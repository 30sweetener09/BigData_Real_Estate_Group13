{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ab062c",
   "metadata": {},
   "source": [
    "### Code để crawl id của từng bds trên trang, limit mỗi page lấy 20 id, chỉnh số page trong phần for page in range(30):\n",
    "Tự đổi path_file cho phù hợp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf885cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "id_path_file = r\"D:\\\\20251\\\\Lưu trữ và xử lý dữ liệu lớn\\\\project\\\\data\\\\all_ids.json\"\n",
    "path_raw_data = r\"D:\\\\20251\\\\Lưu trữ và xử lý dữ liệu lớn\\\\project\\\\data\\\\all_raw_data.json\"\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e01c4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã load 618 ID từ file all_ids.json\n",
      "Trang 31: lấy được 20 ID, thêm mới 20\n",
      "Trang 32: lấy được 20 ID, thêm mới 20\n",
      "Trang 33: lấy được 20 ID, thêm mới 20\n",
      "Trang 34: lấy được 20 ID, thêm mới 19\n",
      "Trang 35: lấy được 20 ID, thêm mới 19\n",
      "Trang 36: lấy được 20 ID, thêm mới 20\n",
      "Trang 37: lấy được 20 ID, thêm mới 20\n",
      "Trang 38: lấy được 20 ID, thêm mới 20\n",
      "Trang 39: lấy được 20 ID, thêm mới 20\n",
      "Trang 40: lấy được 20 ID, thêm mới 19\n",
      "Trang 41: lấy được 20 ID, thêm mới 20\n",
      "Trang 42: lấy được 20 ID, thêm mới 20\n",
      "Trang 43: lấy được 20 ID, thêm mới 20\n",
      "Trang 44: lấy được 20 ID, thêm mới 20\n",
      "Trang 45: lấy được 20 ID, thêm mới 20\n",
      "Trang 46: lấy được 20 ID, thêm mới 20\n",
      "Trang 47: lấy được 20 ID, thêm mới 20\n",
      "Trang 48: lấy được 20 ID, thêm mới 20\n",
      "Trang 49: lấy được 20 ID, thêm mới 20\n",
      "Trang 50: lấy được 20 ID, thêm mới 19\n",
      "Trang 51: lấy được 20 ID, thêm mới 20\n",
      "Trang 52: lấy được 20 ID, thêm mới 19\n",
      "Trang 53: lấy được 20 ID, thêm mới 20\n",
      "Trang 54: lấy được 20 ID, thêm mới 19\n",
      "Trang 55: lấy được 20 ID, thêm mới 20\n",
      "Trang 56: lấy được 20 ID, thêm mới 20\n",
      "Trang 57: lấy được 20 ID, thêm mới 20\n",
      "Trang 58: lấy được 20 ID, thêm mới 19\n",
      "Trang 59: lấy được 20 ID, thêm mới 20\n",
      "Trang 60: lấy được 20 ID, thêm mới 0\n",
      "Đã lưu 1191 ID vào file all_ids.json\n",
      "Tổng cộng: 1191 ad_id duy nhất\n",
      "Một vài ID đầu tiên: [121993216, 128518145, 126779393, 128524301, 128520206, 128514066, 128368658, 128522260, 128442391, 128514074]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    with open(id_path_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_ids = set(json.load(f))  # dùng set để tránh trùng lặp\n",
    "except FileNotFoundError:\n",
    "    all_ids = set()\n",
    "\n",
    "print(f\"Đã load {len(all_ids)} ID từ file all_ids.json\")\n",
    "def get_ad_ids(page, limit=20):\n",
    "    offset = page * limit\n",
    "    url = f\"https://gateway.chotot.com/v1/public/ad-listing?region_v2=12000&cg=1000&o={offset}&limit={limit}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    resp = requests.get(url, headers=headers, timeout=10)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    ads = data.get(\"ads\", [])\n",
    "    ad_ids = [ad[\"list_id\"] for ad in ads if \"list_id\" in ad]\n",
    "    return ad_ids\n",
    "\n",
    "\n",
    "\n",
    "for page in range(30,60):  # crawl 5 trang đầu tiên\n",
    "    ids = get_ad_ids(page)\n",
    "    before = len(all_ids)\n",
    "    all_ids.update(ids)  # thêm và tự loại trùng\n",
    "    print(f\"Trang {page+1}: lấy được {len(ids)} ID, thêm mới {len(all_ids) - before}\")\n",
    "    time.sleep(2)  # tránh bị chặn\n",
    "\n",
    "\n",
    "all_ids_list = list(all_ids)  # nếu là set, convert sang list trước khi lưu\n",
    "\n",
    "# Lưu ra file JSON\n",
    "with open(id_path_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_ids_list, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Đã lưu {len(all_ids_list)} ID vào file all_ids.json\")\n",
    "print(\"Tổng cộng:\", len(all_ids), \"ad_id duy nhất\")\n",
    "print(\"Một vài ID đầu tiên:\", list(all_ids)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62229663",
   "metadata": {},
   "source": [
    "### Code crawl thông tin chi tiết của 1 post dựa vào id đã crawl bên trên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cd8e3f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://gateway.chotot.com/v1/public/ad-listing/128033818",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMô tả:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ad\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m---> 29\u001b[0m \u001b[43mcrawl_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m128033818\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m, in \u001b[0;36mcrawl_data\u001b[1;34m(ad_id, debug)\u001b[0m\n\u001b[0;32m      7\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m }\n\u001b[0;32m     12\u001b[0m resp \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# In đẹp JSON\u001b[39;00m\n",
      "File \u001b[1;32me:\\miniconda\\envs\\van_env\\lib\\site-packages\\requests\\models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1023\u001b[0m     )\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://gateway.chotot.com/v1/public/ad-listing/128033818"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "def crawl_data(ad_id, debug = False):\n",
    "    \n",
    "    url = f\"https://gateway.chotot.com/v1/public/ad-listing/{ad_id}\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "        \"Accept\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    # In đẹp JSON\n",
    "    if debug==True:\n",
    "        print(json.dumps(data, indent=2, ensure_ascii=False))\n",
    "        # Lấy các field quan trọng\n",
    "        ad = data.get(\"ad\")\n",
    "        print(type(data))\n",
    "        print(\"Tiêu đề:\", ad.get(\"subject\"))\n",
    "        print(\"Giá:\", ad.get(\"price\"))\n",
    "        print(\"Diện tích:\", ad.get(\"area\"))\n",
    "        print(\"Địa chỉ:\", ad.get(\"region_name\"), \"-\", ad.get(\"area_name\"))\n",
    "        print(\"Mô tả:\", ad.get(\"body\"))\n",
    "    return data\n",
    "\n",
    "crawl_data(\"128033818\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb187655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã crawl 503 id\n",
      "Còn lại 688 id chưa crawl.\n",
      "❌ ID 128033818 bị lỗi 404 — xóa khỏi danh sách và bỏ qua.\n",
      "Đã lưu 603 bản ghi.\n",
      "Đã lưu 703 bản ghi.\n",
      "Đã lưu 803 bản ghi.\n",
      "❌ ID 128507374 bị lỗi 404 — xóa khỏi danh sách và bỏ qua.\n",
      "Đã lưu 902 bản ghi.\n",
      "Đã lưu 1002 bản ghi.\n",
      "Đã lưu 1102 bản ghi.\n",
      "❌ ID 128511933 bị lỗi 404 — xóa khỏi danh sách và bỏ qua.\n",
      "✅ Crawl hoàn tất. Tổng cộng lưu được 1188 bản ghi.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(id_path_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_ids = set(json.load(f))\n",
    "except FileNotFoundError:\n",
    "    all_ids = set()\n",
    "\n",
    "# --- Đọc dữ liệu đã crawl ---\n",
    "try:\n",
    "    with open(path_raw_data, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_raw_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    all_raw_data = {}\n",
    "\n",
    "crawled_id = set(all_raw_data.keys())\n",
    "print(\"Đã crawl\", len(crawled_id), \"id\")\n",
    "\n",
    "uncrawled_ids = [id for id in all_ids if str(id) not in crawled_id]\n",
    "print(\"Còn lại\", len(uncrawled_ids), \"id chưa crawl.\")\n",
    "\n",
    "for i, id in enumerate(uncrawled_ids):\n",
    "    try:\n",
    "        raw_data = crawl_data(str(id))  # hàm crawl_data của bạn\n",
    "        if raw_data is None:\n",
    "            raise ValueError(\"Không có dữ liệu trả về\")  # để xử lý trường hợp dữ liệu rỗng\n",
    "        all_raw_data[str(id)] = raw_data\n",
    "\n",
    "    except Exception as e:\n",
    "        # Nếu bị lỗi 404 hoặc lỗi nào khác thì bỏ qua id này\n",
    "        err_msg = str(e).lower()\n",
    "        if \"404\" in err_msg or \"not found\" in err_msg:\n",
    "            print(f\"❌ ID {id} bị lỗi 404 — xóa khỏi danh sách và bỏ qua.\")\n",
    "            all_ids.discard(id)\n",
    "            with open(id_path_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(list(all_ids), f, ensure_ascii=False, indent=2)\n",
    "                f.flush()\n",
    "        else:\n",
    "            print(f\"⚠️ Lỗi khi crawl ID {id}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Lưu định kỳ mỗi 100 ID\n",
    "    if i % 100 == 0 and i > 0:\n",
    "        with open(path_raw_data, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_raw_data, f, ensure_ascii=False, indent=2)\n",
    "            f.flush()\n",
    "        print(f\"Đã lưu {len(all_raw_data)} bản ghi.\")\n",
    "\n",
    "    time.sleep(1)  # tránh bị chặn IP\n",
    "\n",
    "# --- Lưu lần cuối ---\n",
    "with open(path_raw_data, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_raw_data, f, ensure_ascii=False, indent=2)\n",
    "    f.flush()\n",
    "\n",
    "print(\"✅ Crawl hoàn tất. Tổng cộng lưu được\", len(all_raw_data), \"bản ghi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "175e4aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{121993216, 128518145, 126779393, 128524301, 128520206, 128514066, 128368658, 128522260, 128442391, 128507930, 128499738, 128514074, 128514079, 127635488, 127586338, 128245794, 128501794, 128014373, 127068203, 127885355, 127393841, 127486003, 128520245, 127723574, 127012920, 128493625, 128516153, 128159803, 128524348, 126427196, 128507968, 128520258, 128481347, 128518212, 124162117, 128262218, 128510030, 127510606, 128174160, 128510032, 127592529, 128520275, 120557649, 128311381, 128514134, 128514135, 127658072, 128514137, 128522330, 126543960, 127635547, 128086111, 126617695, 128520290, 128464995, 128516195, 127422567, 128458857, 128487529, 128432235, 128514156, 127357037, 128520302, 128514159, 128510065, 128508018, 128493686, 128522360, 128444538, 128149628, 128481405, 128161917, 128481406, 122927232, 128030847, 124688515, 128514182, 128155783, 123494536, 128518278, 126232714, 127131787, 127262859, 128520331, 127531163, 128514210, 127658146, 128514213, 123091110, 128516261, 128512168, 125450413, 123287728, 126562480, 128096434, 128518320, 128145590, 128520375, 127938743, 128442553, 128508090, 125884599, 127295676, 128133303, 128477370, 128522434, 124629189, 124821702, 128377031, 128512199, 128508103, 128288969, 128358603, 128514252, 128446668, 126789836, 128518349, 127983825, 128510164, 128514263, 127658200, 128399577, 128522455, 128520409, 128508124, 128352476, 128510174, 128518360, 127660250, 128508129, 128506082, 128516315, 128432360, 128495849, 128516328, 128434411, 128162027, 128289000, 128471278, 127938803, 128518390, 128415993, 128243962, 128522497, 128514307, 128424197, 127656199, 127963399, 128522503, 121526538, 127635723, 125622539, 127811854, 128510223, 128520468, 128520475, 128506143, 128522527, 128360738, 128401699, 128508196, 127953189, 127748387, 127734055, 127588651, 123480363, 126759213, 123633966, 128323886, 127772976, 128512307, 125335860, 128086325, 128463158, 127934775, 128508219, 128518459, 127906109, 128516415, 128397632, 128514375, 128059726, 128512335, 126869840, 128520528, 128510290, 128506194, 128461144, 126245210, 128158042, 128520540, 128512349, 128518493, 128493919, 124625248, 128522603, 127969644, 128508268, 128262509, 128518512, 128495986, 125446517, 127183225, 128176506, 128520575, 128377216, 128110977, 126865794, 128508290, 127938949, 128520582, 128508295, 128518535, 128491919, 128518543, 128442774, 128514458, 128520603, 128463260, 128522652, 127201696, 128518560, 128059812, 123959718, 128518572, 128520624, 128369073, 127492532, 128518580, 127330747, 128508347, 127293895, 128506314, 128522703, 128020947, 127695318, 128508375, 128516566, 127205849, 128401400, 127775195, 125368796, 127885789, 128469468, 128172509, 128010720, 128506336, 128522722, 122667484, 128516575, 128481765, 122200554, 128373227, 128492011, 128522734, 128481774, 128514546, 128520692, 128506359, 128367095, 128514555, 127953405, 128520703, 128506370, 127828484, 127863301, 128438790, 128487943, 127736328, 122401289, 128510476, 128516621, 125600270, 128057871, 126257683, 128453140, 128293397, 120418838, 128424469, 125712920, 122843673, 127685147, 127785499, 128442908, 128360990, 127906335, 124006946, 127586856, 128248363, 127760940, 128522797, 128520750, 128127535, 128457265, 128500273, 128522803, 128502332, 125606461, 122010173, 128518719, 128512576, 124461637, 128508485, 128438855, 128520773, 120187461, 128514634, 128209483, 128506443, 127046221, 128115278, 128293455, 128514649, 128234077, 128361054, 128520797, 128119392, 128037473, 128178786, 128346722, 128516704, 128375397, 128512615, 128209512, 126206567, 128514666, 128467562, 128506473, 128522856, 128346734, 128514671, 127910508, 124146284, 128449141, 128434806, 128520821, 123165306, 128520826, 125751941, 128516741, 127842952, 128514697, 128508555, 128522893, 128512654, 128520847, 128457359, 127761039, 128520850, 128514708, 128455316, 125233814, 128512662, 128348824, 128522907, 127328924, 128518811, 123015842, 128158371, 128381606, 128379560, 128516777, 126554797, 128516781, 128508594, 123249330, 127627956, 128096947, 128518838, 123986610, 128510648, 127075002, 128512699, 128514748, 128516798, 128514753, 128510658, 128123588, 128522948, 128516804, 128508618, 128142027, 123726542, 128514767, 128172751, 121578191, 128522959, 127632083, 128148174, 128506581, 128518870, 128518871, 128508633, 124973785, 128514779, 126071516, 128477913, 128023266, 128514788, 128154341, 128254698, 128520939, 128015083, 128129775, 128508656, 127879921, 126931697, 128488177, 126819056, 128518896, 127593205, 128217846, 126747383, 125811447, 128518903, 128310010, 128508669, 128514814, 118762237, 128506622, 128267009, 128523012, 121803525, 128514822, 128514823, 128402189, 128523022, 128514832, 126282513, 127906578, 128512787, 128518931, 128518928, 127802135, 128428825, 125221657, 128363292, 128512797, 128367390, 128410398, 128512800, 128512801, 128516894, 128252707, 128523043, 128514853, 128518948, 128506663, 122067751, 128516905, 128107306, 128226092, 128523053, 127726381, 127492919, 128340793, 128523065, 128514875, 127310653, 122544960, 124410688, 128516928, 123958083, 128129861, 127759174, 123784005, 128521032, 128516934, 128508748, 125721429, 128516952, 123605850, 128508762, 125377371, 128510811, 128422748, 128506718, 126544733, 117689180, 128041820, 128514914, 128488291, 128506723, 124341093, 128514920, 128512872, 127202152, 127695722, 128508784, 128263026, 128510835, 128379765, 120943478, 128129912, 126806904, 128496506, 128459644, 128519038, 128519044, 128510855, 128519048, 126409610, 126524298, 128517005, 128521102, 127155087, 122989455, 128134032, 128521109, 128523163, 128519068, 127497120, 128514977, 128523172, 128496549, 127837092, 128134053, 128517033, 128517036, 128508845, 128519087, 128508849, 128514994, 128207794, 128060340, 128504755, 128519090, 128500665, 128521146, 127898557, 128521151, 128508871, 128523207, 128369609, 124484556, 128449485, 127456205, 128457680, 127163344, 128523218, 128519123, 127910868, 128519124, 128519128, 128496605, 127120349, 128512991, 128494560, 127319007, 128512995, 128535523, 128422885, 128513002, 128535530, 128515052, 126870507, 128513007, 128515056, 128402416, 128515059, 125561845, 127761398, 128515063, 128510968, 128515064, 127097846, 128459766, 126186487, 128513021, 128519163, 127126518, 125334528, 128523264, 128513032, 128517130, 128506891, 125449229, 128515088, 128506896, 128513042, 127314960, 128504852, 128500754, 128515094, 128508951, 128506903, 127573012, 128033818, 128523291, 128459804, 128484380, 128490522, 125010972, 127575074, 128511011, 128535590, 128521255, 128185384, 128461866, 125334576, 128521265, 128535604, 127624246, 128508983, 127415351, 128523321, 128255034, 128521275, 128431162, 128513086, 128515135, 128521279, 128506945, 128517188, 125811781, 128513096, 128504906, 128519243, 128517197, 128506959, 127446096, 128513104, 128509009, 126340177, 124582996, 128459861, 126708816, 128517203, 127601752, 124984400, 128158812, 126010462, 128511072, 128523361, 128521312, 128160867, 128523364, 126082149, 123276390, 128515174, 128504936, 127446121, 128535658, 127593577, 128517227, 128427117, 128506989, 128517225, 128521328, 111529073, 128494706, 128502899, 128517235, 128515190, 127900792, 128461944, 128521338, 128392317, 128519296, 128388224, 127978624, 128523396, 128519301, 126428298, 128511115, 128513164, 128535693, 128515214, 128535692, 128523407, 128523410, 127913107, 128504980, 122193043, 128515222, 128521366, 128509080, 128515227, 128437403, 128449695, 128507039, 128523425, 128519332, 128421029, 128513198, 128517294, 128519184, 128521394, 128507059, 128523443, 128177333, 126835898, 128523454, 128521406, 128515264, 128515265, 128509122, 126696642, 127923396, 127929541, 127825092, 127728837, 128515276, 128414924, 128488653, 127796431, 128515280, 128513238, 127507670, 128515288, 128247002, 128513245, 125320416, 128515297, 128511202, 128519394, 128535784, 128515307, 128447726, 128290031, 128453872, 128511217, 128437489, 128535793, 128378098, 119686389, 128505078, 128515320, 128517368, 128521466, 123774203, 128519421, 127556862, 128513280, 126031105, 128513282, 128103683, 128505088, 128523524, 128513288, 128509192, 128517385, 128118027, 127956235, 128517390, 128369936, 128509200, 125687061, 128519445, 128519447, 128515352, 128496921, 128513304, 128521496, 128242965, 128515357, 128519453, 127573275, 128519459, 128517413, 128519464, 128515370, 128517418, 128216365, 128519469, 128515375, 127595824, 127114546, 128390450, 128505141, 128517433, 128056636, 128513341, 128251196, 128388412, 128517442, 128517443, 128515396, 128509253, 128519493, 128509255, 128402755, 128519498, 128517450, 128513356, 128519500, 128519503, 128513363, 128511317, 128513366, 127804757, 128515416, 128476501, 127835477, 128515419, 128511324, 127034717, 127950173, 128509283, 128486756, 128517475, 128507240, 128513387, 128515437, 128204142, 128515441, 128460148, 128517495, 128517497, 128515450, 127368570, 128513409, 127374722, 128515459, 128523651, 128462215, 124556680, 128521607, 128507274, 128372108, 128517516, 128408974, 128435599, 127341965, 128337303, 128460185, 128505243, 128509340, 128286110, 128521630, 125986208, 128517536, 128509346, 128226726, 128515495, 128511400, 128368040, 127894957, 127272366, 128517553, 124487089, 128513459, 126537142, 128370104, 128501176, 128517563, 127796669, 128513473, 128515525, 128136646, 128515527, 128513478, 128392646, 128359881, 128507343, 128513488, 126842320, 128509395, 128450003, 128495061, 128517588, 128263636, 128507352, 126363098, 128245212, 128155102, 128511456, 128517601, 126709218, 128474595, 126776801, 128507365, 127299046, 127419879, 128515563, 128507374, 128505327, 128001521, 128521716, 128515574, 127542774, 128511481, 128515579, 127979004, 128515581, 127247871, 128517631, 123149825, 128515586, 123973123, 128509444, 128398852, 126438916, 128146954, 125687306, 128251408, 128091669, 128507414, 128521753, 128515612, 127356445, 128394780, 128517663, 128376352, 126369312, 128523809, 128417313, 125421092, 128507430, 128286248, 128157227, 128515628, 127794732, 127944238, 128056880, 128507443, 125986356, 128515637, 128425525, 128511543, 128374328, 128497207, 128020021, 126815802, 128519733, 128517698, 128515651, 128398918, 128398924, 127694417, 128515666, 127983190, 128515674, 128513626, 128505435, 128513629, 128523866, 128515679, 128112224, 124649052, 128521826, 127051363, 128265831, 128521833, 128515690, 128515694, 128513647, 128521839, 128517746, 128515699, 128515700, 128519797, 128513654, 127368820, 128472696, 126625399, 127993467, 128519804, 128517757, 128515710, 128511614, 128515713, 128507523, 125339269, 123303558, 128515720, 128433801, 128175754, 128507533, 128505486, 128396943, 128515728, 128325265, 128099981, 128513683, 128462483, 128521876, 128480917, 128517783, 128159385, 127848090, 128515739, 128513692, 128505501, 128263838, 125986460, 124548768, 128515746, 126484130, 128261797, 128509606, 127952550, 128515754, 126869168, 128439985, 128229042, 128444084, 128100022, 128161462, 128517814, 128515771, 124925630, 126703299, 128474820, 128505542, 128517831, 128509640, 128511689, 128515786, 127968970, 128345801, 127747786, 128523980, 128423625, 128515792, 122468043, 128501459, 128507604, 128517843, 127596246, 128218839, 128390870, 128356055, 128519898, 128519899, 128521949, 128507614, 128515810, 128513762, 128204516, 128515813, 128507621, 128141033, 128423659, 128515821, 128513773, 128511729, 128517874, 128419570, 128519922, 128515828, 128515829, 128511733, 128456436, 128292600, 128501497, 127956730, 128134906, 128169724, 125402870, 126637817, 126641919, 128315139, 127966980, 127856389, 125630212, 127926024, 128517897, 128507658, 128040714, 112791310, 124430097, 128517909, 128507672, 126701337, 128511771, 123932445, 128513822, 128507678, 127729440, 128089889, 128522020, 128354085, 128511787, 128372524, 127700781, 128481068, 128517937, 128513843, 128517940, 126930741, 127556625, 128261944, 127035193, 127893306, 128126781, 128524094, 128157502, 128513856, 127883077, 126424903, 128395080, 128321353, 124946247, 128081739, 128114510, 128513871, 128524112, 128517968, 123879253, 128438108, 128157534, 128374623, 128520038, 127893357, 128524142, 128520049, 128507763, 128518007, 128505720, 125917050, 128520064, 128513921, 127827841, 127072131, 128401284, 128450437, 128513927, 128487304, 127154060, 128384911, 128417680, 126525329, 128515983, 127430547, 128509844, 127281047, 124526487, 128513945, 126963610, 128522139, 128509857, 128489380, 128513957, 127561641, 128448427, 128522156, 127866797, 128505774, 123996081, 128520114, 128524217, 128227259, 128511933, 128518077, 125763520, 127762369, 128522178, 128489411, 123840449, 128513989, 128053190, 126760901, 128403400, 128219082, 128522186, 127829964, 128509900, 127932372, 128520148, 128503765, 128180184, 128505816, 128501722, 123371480, 128516061, 128518110, 128520160, 128509921, 128159712, 127715299, 128456676, 128518117, 128268262, 128430055, 128520167, 127862761, 128514026, 128428009, 128409580, 128511983, 128522226, 128491507, 128518130, 128518131, 128520182, 128514039, 128509944, 127389690, 128520187, 128516094}\n"
     ]
    }
   ],
   "source": [
    "print(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a1f537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Eclipse Adoptium\\jdk-11.0.28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Eclipse Adoptium\\\\jdk-11.0.28\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"\\\\bin;\" + os.environ[\"PATH\"]\n",
    "print(os.environ.get(\"JAVA_HOME\"))\n",
    "print(os.popen(\"java -version\").read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b4f0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(r\"D:\\\\20251\\\\Lưu trữ và xử lý dữ liệu lớn\\\\project\\\\github\\\\BigData_Real_Estate_Group13\\\\streaming_input\\\\all_raw_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(\"data_lines.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for key, value in data.items():\n",
    "        obj = {\"id\": key, \"data\": value}\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6df0465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\miniconda\\\\envs\\\\spark_env\\\\Lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86373e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "\n",
    "import os\n",
    "\n",
    "# Tắt Hadoop native libraries\n",
    "os.environ['HADOOP_HOME'] = ''\n",
    "os.environ['HADOOP_CONF_DIR'] = ''\n",
    "\n",
    "# Hoặc set property này\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StructuredStreamingExample\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.hadoop.io.nativeio.windows\", \"false\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "098e2800",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o50.start.\n: java.net.ConnectException: Call From DESKTOP-17NN6TB/172.20.10.2 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused: getsockopt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\r\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:961)\r\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:876)\r\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1529)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1426)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139)\r\n\tat jdk.proxy2/jdk.proxy2.$Proxy47.getFileInfo(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.lambda$getFileInfo$41(ClientNamenodeProtocolTranslatorPB.java:820)\r\n\tat org.apache.hadoop.ipc.internal.ShadedProtobufHelper.ipc(ShadedProtobufHelper.java:160)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:820)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:437)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:170)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:162)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:100)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:366)\r\n\tat jdk.proxy2/jdk.proxy2.$Proxy48.getFileInfo(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1770)\r\n\tat org.apache.hadoop.fs.Hdfs.getFileStatus(Hdfs.java:149)\r\n\tat org.apache.hadoop.fs.FileContext$15.next(FileContext.java:1255)\r\n\tat org.apache.hadoop.fs.FileContext$15.next(FileContext.java:1251)\r\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\r\n\tat org.apache.hadoop.fs.FileContext.getFileStatus(FileContext.java:1257)\r\n\tat org.apache.hadoop.fs.FileContext$Util.exists(FileContext.java:1760)\r\n\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.exists(CheckpointFileManager.scala:331)\r\n\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.<init>(HDFSMetadataLog.scala:69)\r\n\tat org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.<init>(CompactibleFileStreamLog.scala:49)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSinkLog.<init>(FileStreamSinkLog.scala:91)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink.<init>(FileStreamSink.scala:144)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createSink(DataSource.scala:337)\r\n\tat org.apache.spark.sql.classic.DataStreamWriter.createV1Sink(DataStreamWriter.scala:335)\r\n\tat org.apache.spark.sql.classic.DataStreamWriter.startInternal(DataStreamWriter.scala:288)\r\n\tat org.apache.spark.sql.classic.DataStreamWriter.start(DataStreamWriter.scala:136)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.ConnectException: Connection refused: getsockopt\r\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\r\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:682)\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:973)\r\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)\r\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:614)\r\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)\r\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:789)\r\n\tat org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:364)\r\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1473)\r\n\t... 45 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 49\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Parse JSON nested\u001b[39;00m\n\u001b[0;32m     40\u001b[0m processed_df \u001b[38;5;241m=\u001b[39m stream_df\u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m     41\u001b[0m     col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist_id\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     42\u001b[0m     col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.ad.*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[43mprocessed_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdfs://localhost:9000/streaming/realestate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdfs://localhost:9000/checkpoints/realestate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 49\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[1;32me:\\miniconda\\envs\\spark_env\\lib\\site-packages\\pyspark\\sql\\streaming\\readwriter.py:1704\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[1;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[1;32me:\\miniconda\\envs\\spark_env\\lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32me:\\miniconda\\envs\\spark_env\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32me:\\miniconda\\envs\\spark_env\\lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o50.start.\n: java.net.ConnectException: Call From DESKTOP-17NN6TB/172.20.10.2 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused: getsockopt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\r\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:961)\r\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:876)\r\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1529)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1426)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139)\r\n\tat jdk.proxy2/jdk.proxy2.$Proxy47.getFileInfo(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.lambda$getFileInfo$41(ClientNamenodeProtocolTranslatorPB.java:820)\r\n\tat org.apache.hadoop.ipc.internal.ShadedProtobufHelper.ipc(ShadedProtobufHelper.java:160)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:820)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:437)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:170)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:162)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:100)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:366)\r\n\tat jdk.proxy2/jdk.proxy2.$Proxy48.getFileInfo(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1770)\r\n\tat org.apache.hadoop.fs.Hdfs.getFileStatus(Hdfs.java:149)\r\n\tat org.apache.hadoop.fs.FileContext$15.next(FileContext.java:1255)\r\n\tat org.apache.hadoop.fs.FileContext$15.next(FileContext.java:1251)\r\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\r\n\tat org.apache.hadoop.fs.FileContext.getFileStatus(FileContext.java:1257)\r\n\tat org.apache.hadoop.fs.FileContext$Util.exists(FileContext.java:1760)\r\n\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.exists(CheckpointFileManager.scala:331)\r\n\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.<init>(HDFSMetadataLog.scala:69)\r\n\tat org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.<init>(CompactibleFileStreamLog.scala:49)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSinkLog.<init>(FileStreamSinkLog.scala:91)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink.<init>(FileStreamSink.scala:144)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createSink(DataSource.scala:337)\r\n\tat org.apache.spark.sql.classic.DataStreamWriter.createV1Sink(DataStreamWriter.scala:335)\r\n\tat org.apache.spark.sql.classic.DataStreamWriter.startInternal(DataStreamWriter.scala:288)\r\n\tat org.apache.spark.sql.classic.DataStreamWriter.start(DataStreamWriter.scala:136)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.ConnectException: Connection refused: getsockopt\r\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\r\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:682)\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:973)\r\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)\r\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:614)\r\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)\r\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:789)\r\n\tat org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:364)\r\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1473)\r\n\t... 45 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, BooleanType\n",
    "from pyspark.sql.functions import col, from_json\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RealEstateJSONtoHDFS\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Định nghĩa schema JSON\n",
    "ad_schema = StructType([\n",
    "    StructField(\"ad_id\", LongType(), True),\n",
    "    StructField(\"list_id\", LongType(), True),\n",
    "    StructField(\"list_time\", LongType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"account_name\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"region\", IntegerType(), True),\n",
    "    StructField(\"category\", IntegerType(), True),\n",
    "    StructField(\"company_ad\", BooleanType(), True),\n",
    "    StructField(\"subject\", StringType(), True),\n",
    "    StructField(\"body\", StringType(), True)\n",
    "])\n",
    "\n",
    "data_schema = StructType([StructField(\"ad\", ad_schema, True)])\n",
    "main_schema = StructType([StructField(\"id\", StringType(), True),\n",
    "                          StructField(\"data\", data_schema, True)])\n",
    "\n",
    "# Thư mục chứa JSON streaming\n",
    "input_path = r\"D:\\\\20251\\\\Lưu trữ và xử lý dữ liệu lớn\\\\project\\\\github\\\\BigData_Real_Estate_Group13\\\\streaming_input\"\n",
    "input_path = input_path.replace(\"\\\\\",\"/\")\n",
    "\n",
    "stream_df = spark.readStream \\\n",
    "    .schema(main_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .json(input_path)\n",
    "\n",
    "# Parse JSON nested\n",
    "processed_df = stream_df.select(\n",
    "    col(\"id\").alias(\"list_id\"),\n",
    "    col(\"data.ad.*\")\n",
    ")\n",
    "processed_df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"hdfs://localhost:9000/streaming/realestate\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:9000/checkpoints/realestate\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start() \\\n",
    "    .awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b63560a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-17NN6TB:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FileStreamingTest</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x276deda6aa0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c3da149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)  # Ví dụ: 3.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85d8b73c",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o34.json.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:218)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:132)\r\n\tat scala.collection.immutable.List.map(List.scala:247)\r\n\tat scala.collection.immutable.List.map(List.scala:79)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:122)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:72)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:179)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:135)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:563)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$sourceSchema$2(DataSource.scala:279)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$lzycompute$1(DataSource.scala:173)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$1(DataSource.scala:173)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:178)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:273)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:129)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:129)\r\n\tat org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:37)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:119)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\r\n\tat org.apache.spark.sql.classic.DataStreamReader.loadInternal(DataStreamReader.scala:81)\r\n\tat org.apache.spark.sql.classic.DataStreamReader.load(DataStreamReader.scala:90)\r\n\tat org.apache.spark.sql.classic.DataStreamReader.load(DataStreamReader.scala:41)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.json(DataStreamReader.scala:143)\r\n\tat org.apache.spark.sql.classic.DataStreamReader.json(DataStreamReader.scala:135)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m     11\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType() \\\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType()) \\\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, StructType())\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Đọc luồng dữ liệu\u001b[39;00m\n\u001b[0;32m     16\u001b[0m stream_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Hiển thị dữ liệu ra console\u001b[39;00m\n\u001b[0;32m     21\u001b[0m query \u001b[38;5;241m=\u001b[39m stream_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[1;32me:\\miniconda\\envs\\spark_env\\lib\\site-packages\\pyspark\\sql\\streaming\\readwriter.py:415\u001b[0m, in \u001b[0;36mDataStreamReader.json\u001b[1;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, locale, dropFieldIfAllNull, encoding, pathGlobFilter, recursiveFileLookup, allowNonNumericNumbers, useUnsafeRow)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m    391\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[0;32m    392\u001b[0m     primitivesAsString\u001b[38;5;241m=\u001b[39mprimitivesAsString,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m     useUnsafeRow\u001b[38;5;241m=\u001b[39museUnsafeRow,\n\u001b[0;32m    413\u001b[0m )\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    418\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    419\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(path)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    420\u001b[0m     )\n",
      "File \u001b[1;32me:\\miniconda\\envs\\spark_env\\lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32me:\\miniconda\\envs\\spark_env\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32me:\\miniconda\\envs\\spark_env\\lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o34.json.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:218)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:132)\r\n\tat scala.collection.immutable.List.map(List.scala:247)\r\n\tat scala.collection.immutable.List.map(List.scala:79)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:122)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:72)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:179)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:135)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:563)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$sourceSchema$2(DataSource.scala:279)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$lzycompute$1(DataSource.scala:173)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$1(DataSource.scala:173)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:178)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:273)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:129)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:129)\r\n\tat org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:37)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:119)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\r\n\tat org.apache.spark.sql.classic.DataStreamReader.loadInternal(DataStreamReader.scala:81)\r\n\tat org.apache.spark.sql.classic.DataStreamReader.load(DataStreamReader.scala:90)\r\n\tat org.apache.spark.sql.classic.DataStreamReader.load(DataStreamReader.scala:41)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.json(DataStreamReader.scala:143)\r\n\tat org.apache.spark.sql.classic.DataStreamReader.json(DataStreamReader.scala:135)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType\n",
    "\n",
    "\n",
    "\n",
    "# Đường dẫn đến thư mục chứa file JSON\n",
    "input_path = r\"D:\\\\20251\\\\Lưu trữ và xử lý dữ liệu lớn\\\\project\\\\github\\\\BigData_Real_Estate_Group13\\\\streaming_input\"  # Thay đổi đường dẫn này\n",
    "\n",
    "# Định nghĩa schema\n",
    "schema = StructType() \\\n",
    "    .add(\"id\", IntegerType()) \\\n",
    "    .add(\"data\", StructType())\n",
    "\n",
    "# Đọc luồng dữ liệu\n",
    "stream_df = spark.readStream \\\n",
    "    .schema(schema) \\\n",
    "    .json(input_path)\n",
    "\n",
    "# Hiển thị dữ liệu ra console\n",
    "query = stream_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Chờ query kết thúc\n",
    "query.awaitTermination()\n",
    "\n",
    "# Hoặc chạy trong khoảng thời gian giới hạn\n",
    "# query.awaitTermination(timeout=60)  # 60 giây"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82367f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o46.json.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:218)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:132)\r\n\tat scala.collection.immutable.List.map(List.scala:247)\r\n\tat scala.collection.immutable.List.map(List.scala:79)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:122)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:72)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:179)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:135)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:563)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$sourceSchema$2(DataSource.scala:279)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$lzycompute$1(DataSource.scala:173)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$1(DataSource.scala:173)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:178)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:273)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:129)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:129)\r\n\tat org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:37)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:119)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\r\n\tat org.apache.spark.sql.classic.DataStreamReader.loadInternal(DataStreamReader.scala:81)\r\n\tat org.apache.spark.sql.classic.DataStreamReader.load(DataStreamReader.scala:90)\r\n\tat org.apache.spark.sql.classic.DataStreamReader.load(DataStreamReader.scala:41)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.json(DataStreamReader.scala:143)\r\n\tat org.apache.spark.sql.classic.DataStreamReader.json(DataStreamReader.scala:135)\r\n\tat org.apache.spark.sql.classic.DataStreamReader.json(DataStreamReader.scala:41)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m      5\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType() \\\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType()) \\\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, StructType())\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Đọc luồng dữ liệu - This should now pass the native check\u001b[39;00m\n\u001b[0;32m     10\u001b[0m stream_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Hiển thị dữ liệu ra console\u001b[39;00m\n\u001b[0;32m     15\u001b[0m query \u001b[38;5;241m=\u001b[39m stream_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[1;32me:\\miniconda\\envs\\spark_env\\lib\\site-packages\\pyspark\\sql\\streaming\\readwriter.py:415\u001b[0m, in \u001b[0;36mDataStreamReader.json\u001b[1;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, locale, dropFieldIfAllNull, encoding, pathGlobFilter, recursiveFileLookup, allowNonNumericNumbers, useUnsafeRow)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m    391\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[0;32m    392\u001b[0m     primitivesAsString\u001b[38;5;241m=\u001b[39mprimitivesAsString,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m     useUnsafeRow\u001b[38;5;241m=\u001b[39museUnsafeRow,\n\u001b[0;32m    413\u001b[0m )\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    418\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    419\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(path)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    420\u001b[0m     )\n",
      "File \u001b[1;32me:\\miniconda\\envs\\spark_env\\lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32me:\\miniconda\\envs\\spark_env\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32me:\\miniconda\\envs\\spark_env\\lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o46.json.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:218)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:132)\r\n\tat scala.collection.immutable.List.map(List.scala:247)\r\n\tat scala.collection.immutable.List.map(List.scala:79)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:122)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:72)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:179)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:135)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:563)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$sourceSchema$2(DataSource.scala:279)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$lzycompute$1(DataSource.scala:173)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$1(DataSource.scala:173)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:178)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:273)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:129)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:129)\r\n\tat org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:37)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:119)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\r\n\tat org.apache.spark.sql.classic.DataStreamReader.loadInternal(DataStreamReader.scala:81)\r\n\tat org.apache.spark.sql.classic.DataStreamReader.load(DataStreamReader.scala:90)\r\n\tat org.apache.spark.sql.classic.DataStreamReader.load(DataStreamReader.scala:41)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.json(DataStreamReader.scala:143)\r\n\tat org.apache.spark.sql.classic.DataStreamReader.json(DataStreamReader.scala:135)\r\n\tat org.apache.spark.sql.classic.DataStreamReader.json(DataStreamReader.scala:41)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ... rest of your code ...\n",
    "# Đọc dữ liệu streaming từ folder\n",
    "input_path = \"D:/20251/Lưu trữ và xử lý dữ liệu lớn/project/github/BigData_Real_Estate_Group13/streaming_input\"\n",
    "\n",
    "schema = StructType() \\\n",
    "    .add(\"id\", IntegerType()) \\\n",
    "    .add(\"data\", StructType())\n",
    "\n",
    "# Đọc luồng dữ liệu - This should now pass the native check\n",
    "stream_df = spark.readStream \\\n",
    "    .schema(schema) \\\n",
    "    .json(input_path)\n",
    "\n",
    "# Hiển thị dữ liệu ra console\n",
    "query = stream_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
